1
00:00:00,400 --> 00:00:02,866
300行代码带你理解GRPO算法原理

2
00:00:02,866 --> 00:00:05,166
然后通过GRPO强化学习算法

3
00:00:05,166 --> 00:00:06,800
提升模型的agent的能力

4
00:00:06,800 --> 00:00:08,000
我是曹发

5
00:00:08,000 --> 00:00:10,866
有人说如果做agent你不会强化学习

6
00:00:10,866 --> 00:00:12,500
那么你就被AI时代抛弃了

7
00:00:12,500 --> 00:00:14,266
但是我并不认同这个看法

8
00:00:14,266 --> 00:00:16,766
因为我们做agent的话

9
00:00:16,766 --> 00:00:18,933
并不是只做强化学习这一部分

10
00:00:18,933 --> 00:00:20,500
但是这个观点

11
00:00:20,500 --> 00:00:23,100
它其实也有它的一个很重要的作用

12
00:00:23,100 --> 00:00:25,966
因为强化学习在工作中确实非常重要

13
00:00:25,966 --> 00:00:27,466
尤其是在agent中

14
00:00:27,466 --> 00:00:28,100
所以今天

15
00:00:28,100 --> 00:00:30,000
我会带大家先来理解一下

16
00:00:30,000 --> 00:00:31,866
g r p o的算法基础知识

17
00:00:31,866 --> 00:00:34,266
然后用代码实现一个相对来说

18
00:00:34,266 --> 00:00:35,533
比较有代表性的agent

19
00:00:35,566 --> 00:00:36,733
RL的项目

20
00:00:36,733 --> 00:00:38,333
让大家能够在工作中

21
00:00:38,333 --> 00:00:40,300
把强化学习给用起来

22
00:00:41,266 --> 00:00:42,933
okay接下来给大家讲解一下

23
00:00:42,933 --> 00:00:44,066
什么是GRPO算法

24
00:00:44,066 --> 00:00:45,600
我们通过回答5个问题

25
00:00:45,600 --> 00:00:46,666
来回答这个问题

26
00:00:46,666 --> 00:00:47,700
首先是第一个问题

27
00:00:47,700 --> 00:00:48,800
什么是强化学习

28
00:00:48,800 --> 00:00:49,800
强化学习指的是

29
00:00:49,800 --> 00:00:52,233
一个agent跟环境做多轮的交互

30
00:00:52,233 --> 00:00:55,400
来满足最终的奖励最大化

31
00:00:55,400 --> 00:00:56,500
那么什么是奖励

32
00:00:56,500 --> 00:00:58,466
奖励是指他完成任务之后

33
00:00:58,466 --> 00:01:00,933
人或者说环境给他的打分

34
00:01:01,666 --> 00:01:04,066
他希望这个分数是越高越好

35
00:01:04,900 --> 00:01:05,866
然后第二个问题是

36
00:01:05,866 --> 00:01:07,666
我们要理解什么是PPO算法

37
00:01:07,666 --> 00:01:09,000
刚刚说到的强化学习

38
00:01:09,000 --> 00:01:11,533
它主要的目标是让奖励最大化

39
00:01:11,533 --> 00:01:12,633
然后PPO算法的话

40
00:01:12,633 --> 00:01:15,300
它是实现这样子的一种方式

41
00:01:15,300 --> 00:01:17,133
那么我们可以看一下这张图

42
00:01:17,133 --> 00:01:19,633
它一共有多少个model

43
00:01:19,633 --> 00:01:21,466
一共有四个model policy

44
00:01:21,466 --> 00:01:24,500
model以及reference reward以及value model

45
00:01:24,533 --> 00:01:27,000
然后为什么要这么做呢

46
00:01:27,000 --> 00:01:28,833
是因为policy gradient

47
00:01:28,833 --> 00:01:29,933
它的那个方差比较大

48
00:01:29,933 --> 00:01:32,700
所以说采用了这种actor critic的模式

49
00:01:32,700 --> 00:01:34,033
那么这个actor的话

50
00:01:34,033 --> 00:01:35,533
指的就是这个policy model

51
00:01:35,533 --> 00:01:38,233
然后critic的话指的就是这个value model

52
00:01:38,333 --> 00:01:39,266
然后

53
00:01:39,266 --> 00:01:41,866
你的那个policy model去做了一次生成

54
00:01:41,866 --> 00:01:44,266
之后我们去算loss的话

55
00:01:44,266 --> 00:01:45,833
就是算它的那个KL散度

56
00:01:45,833 --> 00:01:47,433
然后以及

57
00:01:47,433 --> 00:01:50,466
你通过阶跃算法实现的它那个advantage

58
00:01:50,466 --> 00:01:52,066
那么它这个advantage的话

59
00:01:52,066 --> 00:01:55,466
可以理解为就是替代那个reward model的

60
00:01:55,466 --> 00:01:56,866
这里我们不细究啊

61
00:01:56,866 --> 00:01:58,000
你就只要理解为

62
00:01:58,000 --> 00:02:00,333
它就是一种特殊的reward model就行了

63
00:02:00,333 --> 00:02:02,466
因为我们这个不是来讲强化学习

64
00:02:02,466 --> 00:02:04,700
不是来讲PPO算法的

65
00:02:04,700 --> 00:02:09,433
OK，然后上面我们刚刚说到的

66
00:02:09,433 --> 00:02:11,833
其实它都是一个

67
00:02:11,833 --> 00:02:13,233
目标是奖励最大化

68
00:02:13,233 --> 00:02:15,233
但它实际上一般是期望表达形式

69
00:02:15,233 --> 00:02:16,500
不过在训练的过程中

70
00:02:16,500 --> 00:02:17,900
是一个loss的表达形式

71
00:02:17,900 --> 00:02:21,333
这个待会在代码中我们会给大家讲到

72
00:02:21,333 --> 00:02:23,266
那么什么是GRPO算法呢

73
00:02:23,266 --> 00:02:24,266
GRPO算法的话

74
00:02:24,266 --> 00:02:26,433
指的是它去掉了这个value model

75
00:02:26,433 --> 00:02:27,200
可以看到

76
00:02:27,200 --> 00:02:29,700
对于呃相对于PPO来说

77
00:02:29,700 --> 00:02:31,200
它是去掉了这个value model的

78
00:02:31,200 --> 00:02:32,800
然后它每一次生成的话

79
00:02:32,800 --> 00:02:34,400
就不只是生成一条回复了

80
00:02:34,400 --> 00:02:35,933
而是生成多条回复

81
00:02:36,000 --> 00:02:39,200
然后再去计算它组内的那个advantage

82
00:02:39,500 --> 00:02:40,400
呃这样子的话

83
00:02:40,400 --> 00:02:41,833
不知道大家可不可以理解啊

84
00:02:41,833 --> 00:02:43,000
如果大家不可以理解的话

85
00:02:43,000 --> 00:02:44,200
还可以来看一下这张图

86
00:02:44,200 --> 00:02:45,433
我们对比着来看

87
00:02:45,500 --> 00:02:48,300
就是我们有了一一个question

88
00:02:48,300 --> 00:02:49,500
也就是一个

89
00:02:49,500 --> 00:02:51,800
prompt它去输出4条回复

90
00:02:51,800 --> 00:02:53,100
然后每一条回复的话

91
00:02:53,100 --> 00:02:54,433
都会记上一个reward

92
00:02:54,433 --> 00:02:56,500
然后计算了reward之后

93
00:02:56,500 --> 00:03:00,633
它去计算每一个每一条回复的advantage

94
00:03:01,033 --> 00:03:02,700
它是呃

95
00:03:02,800 --> 00:03:05,333
通过这一条回复减去一个均值

96
00:03:05,333 --> 00:03:06,333
然后再除以方差

97
00:03:06,333 --> 00:03:08,333
就得到了这一条回复的advantage

98
00:03:08,333 --> 00:03:10,533
是不是刚刚回答了刚刚那个问题

99
00:03:10,533 --> 00:03:11,533
就advantage的话

100
00:03:11,533 --> 00:03:12,100
可以理解为

